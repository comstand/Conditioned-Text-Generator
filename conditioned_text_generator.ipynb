{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "finalProject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "160GU95kZK73"
      },
      "source": [
        "## MPCS 53113\n",
        "## Final Project\n",
        "## Conditioned Text Generation\n",
        "## By: Andrew Comstock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNrkIyenxJ1D",
        "colab_type": "text"
      },
      "source": [
        "## Project layout:\n",
        "- 1. Define the Environment\n",
        "- 2. Define the Model\n",
        "- 3. Train the Model\n",
        "- 4. Test the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vhBgggJkZK8G"
      },
      "source": [
        "## 1. Create the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XRaLBei4ZK8K",
        "outputId": "4b5d0e3e-2a30-4aac-d10b-0f88a75db707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "import torch.utils.data as tud\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter, defaultdict\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import operator\n",
        "import os, math\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from torchtext.vocab import Vectors\n",
        "from itertools import chain\n",
        "\n",
        "# Set up training enviornment\n",
        "BATCH_SIZE = 32\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SEED = 9432\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set up the dataset we will use to train the generator\n",
        "TEXT = data.Field(tokenize = 'spacy', lower=True, eos_token=\"<eos>\", init_token=\"<init>\")\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "disc_train_data, disc_test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "disc_train_data, disc_valid_data = disc_train_data.split(random_state = random.seed(SEED))\n",
        "\n",
        "TEXT.build_vocab(disc_train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(disc_train_data)\n",
        "VOCAB_SIZE_DISC = len(TEXT.vocab)\n",
        "\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "\n",
        "# Split the data into a train, valid, and test set\n",
        "disc_train_iterator, disc_valid_iterator, disc_test_iterator = data.BucketIterator.splits(\n",
        "    (disc_train_data, disc_valid_data, disc_test_data), \n",
        "    batch_size = BATCH_SIZE, device = device)\n",
        "\n",
        "# setup model parameters\n",
        "INPUT_DIM = VOCAB_SIZE\n",
        "EMBEDDING_DIM = 50\n",
        "CONTEXT_DIM = 1\n",
        "LATENT_DIM = 30\n",
        "HIDDEN_DIM = 100\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "VAE_PATH = 'VAE-model.pt'"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up Vocab\n",
            "Setting up Iterators\n",
            "Done with setup!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81oZov-bZJXr",
        "colab": {}
      },
      "source": [
        "# Define several commonly used functions\n",
        "def lookup_indexes(x, vocab=TEXT.vocab):\n",
        "    ''' \n",
        "    lokoup_indexes takes as input a iterable of strings and an optional\n",
        "    vocabulary, and returns the vocabulary indexes of those words\n",
        "\n",
        "    x: iterable of strings\n",
        "    vocab: vocabulary item\n",
        "\n",
        "    returns: list of integers\n",
        "    '''\n",
        "    if vocab is not None:\n",
        "        x = [vocab.stoi[i] for i in x]\n",
        "    return [t for t in x]\n",
        "    \n",
        "def lookup_words(x, vocab=TEXT.vocab):\n",
        "    '''\n",
        "    lookup_words takes as in put an iterable of integers and an optional\n",
        "    vocabulary, and returns the vocabulary word at that integer index\n",
        "\n",
        "    x: iterable of integers\n",
        "    vocab: vocabulary item\n",
        "\n",
        "    returns: list of strings\n",
        "    '''\n",
        "    if vocab is not None:\n",
        "        x = [vocab.itos[i] for i in x]\n",
        "    return [str(t) for t in x]\n",
        "\n",
        "def makeStarter(s, device, random=False, starter=True):\n",
        "    '''\n",
        "    makeStarter takes a string and device and returns a tensor of\n",
        "    vocabulary indices of the form required for the Encoder, Decoder, and Generator\n",
        "    \n",
        "    s: a string if random is false.\n",
        "       If random is true, s is an integer representing the number of random\n",
        "       words to create.\n",
        "    device: A tensor device, either CPU or CUDA\n",
        "    random: True or False. If True, instead of creating a tensor from a string,\n",
        "            it generates a tensor with s random words from the dictionary.\n",
        "    starter: If True, the first element in the return will be\n",
        "             the start of sentence tag.\n",
        "    \n",
        "    returns: A 1D tensor of type device which can be input to the model\n",
        "    \n",
        "    NOTE: In the vocabulary, words and puncuation are seperated by a space. \n",
        "          For instance \"Great!\" should be \"Great !\".\n",
        "    '''\n",
        "    if starter:\n",
        "        starter = torch.Tensor(lookup_indexes([\"<init>\"])).long().reshape((1,1)).to(device)\n",
        "    else:\n",
        "        starter = torch.Tensor().long().to(device)\n",
        "        \n",
        "    if random:\n",
        "        for i in range(s):\n",
        "            nextWord = torch.randint(INPUT_DIM, (1,1), dtype=torch.long).to(device)\n",
        "            starter = torch.cat([starter, nextWord], dim=0)\n",
        "    else:\n",
        "        arr = s.split(\" \")\n",
        "        for s in arr:\n",
        "            nextWord = torch.Tensor(lookup_indexes([s.lower()])).long().reshape((1,1)).to(device)\n",
        "            starter = torch.cat([starter, nextWord], dim=0)\n",
        "    return starter\n",
        "\n",
        "def makeTarget(target, device, vocab_len, pad_idx=1, max_len=None):\n",
        "    '''\n",
        "    makeTarget takes a tensor of vocabulary words and modifies it to\n",
        "    the target form for training the VAE.\n",
        "    \n",
        "    target: ND tensor of type device\n",
        "    device: pytorch device.\n",
        "    vocab_len: The size of the vocabulary\n",
        "    pad_idx: vocabulary index of the pad character.\n",
        "    \n",
        "    returns: A tensor of the same shape and size of the original, where\n",
        "             each element is shifted to the left by one. With a pad character\n",
        "             appended to the end.\n",
        "    '''\n",
        "    batch_size = target.shape[1]\n",
        "    pad_word = torch.Tensor([pad_idx]).long().repeat(1, batch_size).to(device)\n",
        "    target = torch.cat([target[1:], pad_word], dim=0)\n",
        "    return target.long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AEWUKM_mZK8i"
      },
      "source": [
        "## 2. Define the System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx9YnyOUwv5h",
        "colab_type": "text"
      },
      "source": [
        "Encoder Module:\n",
        "    This is used for both the encoder and the discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-TrjkzaZK8m",
        "colab": {}
      },
      "source": [
        "class RNNBinaryEncoder(nn.Module):\n",
        "    '''\n",
        "    RNNBinaryEncoder is a encoder module capable of producing\n",
        "    context and latent vectors for encoding and sentiment analysis.\n",
        "    \n",
        "    This module is used for both the Discriminator and the Encoder.\n",
        "    \n",
        "    For Discriminator operaiton, the latent dimensions are set to zero.\n",
        "    '''\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, context_dim, latent_dim, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.latent_dim = latent_dim        \n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        \n",
        "        # Linear layers for the context and latent layers\n",
        "        self.fcout = nn.Linear(hidden_dim, context_dim)\n",
        "        self.fcmu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fcsig = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, text, usedAsDiscriminator=False):\n",
        "        '''\n",
        "        usedAsDiscriminator: Boolean. If true, then the output is binary. IE.\n",
        "        the output is 0 or 1.\n",
        "        '''\n",
        "        # Run the text through the embeddings\n",
        "        embedded = self.embedding(text)\n",
        "        meaned = torch.mean(embedded, dim=0, keepdim=True)\n",
        "        \n",
        "        # Run the embeddings through the RNN\n",
        "        _, hidden = self.rnn(meaned)\n",
        "        \n",
        "        # Find the context\n",
        "        context = self.fcout(hidden.squeeze(0))\n",
        "        latent = None\n",
        "        mu = None\n",
        "        sig = None\n",
        "        \n",
        "        # If usedAsDiscriminator, set the output to either 0 or 1\n",
        "        if usedAsDiscriminator:\n",
        "            context[context < 0.5] = 0\n",
        "            context[context >= 0.5] = 1\n",
        "        # Otherwise, find the latent vector\n",
        "        else:\n",
        "            mu = torch.abs(self.fcmu(hidden.squeeze(0)))\n",
        "            sig = torch.abs(self.fcsig(hidden.squeeze(0)))\n",
        "            latent = self.make_latent(mu, sig)\n",
        "\n",
        "        # Returns mu and sigma for calculating the loss\n",
        "        return context, latent, mu, sig\n",
        "    \n",
        "    def make_latent(self, mu, sig):\n",
        "        '''\n",
        "        Make the latent vector using mu, sigma, and sample of the normal distribution\n",
        "        '''\n",
        "        # latent = mu + sigma^(1/2) * (unit guassian)\n",
        "        gauss = torch.randn(self.latent_dim).to(self.device)\n",
        "        return torch.abs(mu + torch.exp(sig/2) * gauss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e_hF9-6PZK81"
      },
      "source": [
        "Decoder module: This is used for the Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ynNayDRBZK82",
        "colab": {}
      },
      "source": [
        "class GRUDecoder(nn.Module):\n",
        "    '''\n",
        "    GRUDecoder is the decoder for the VAE module\n",
        "    Decodes input text, with a given context and latent vector\n",
        "    '''\n",
        "    def __init__(self, output_dim, embedding_dim, context_dim, latent_dim, hidden_dim, num_layers=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(context_dim + latent_dim + embedding_dim, hidden_dim, num_layers)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, context, latent, hidden=None):\n",
        "        length = input.shape[0]\n",
        "        batch_size = input.shape[1]\n",
        "        \n",
        "        # Create the embedding and context\n",
        "        other = torch.cat([context.unsqueeze(0), latent.unsqueeze(0)],dim=2)\n",
        "        input = self.embedding(input)\n",
        "        \n",
        "        input = torch.cat([input, other.repeat(length, 1, 1)],dim=2)\n",
        "        \n",
        "        # Run through the network\n",
        "        output, hidden = self.rnn(input, hidden)\n",
        "        \n",
        "        length, batch_size, _ = output.shape\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4Vu1oTPZK9B"
      },
      "source": [
        "Generator model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fW_FCSvZK9D",
        "colab": {}
      },
      "source": [
        "class EncDec(nn.Module):\n",
        "    '''\n",
        "    EncDec is the main model. It contains the encoder, decoder, discrminator\n",
        "    and generation.\n",
        "    '''\n",
        "    def __init__(self, device, discriminator, vocab_size, embedding_dim, hidden_dim, context_dim, latent_dim, num_layers=1, unk_idx=0, pad_idx=1, start_idx=2, eos_idx=3):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Default vocabulary definitions for generations\n",
        "        self.unk_idx = unk_idx\n",
        "        self.pad_idx = pad_idx\n",
        "        self.start_idx = start_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        \n",
        "        self.encoder = RNNBinaryEncoder(vocab_size, embedding_dim, hidden_dim, context_dim, latent_dim, device).to(device)\n",
        "        \n",
        "        self.decoder = GRUDecoder(vocab_size, embedding_dim, context_dim, latent_dim, hidden_dim, num_layers=num_layers).to(device)\n",
        "        \n",
        "        self.discriminator = discriminator.to(device)\n",
        "        \n",
        "        # Group the model parameters for training\n",
        "        self.param_enc = chain(self.encoder.parameters())\n",
        "        self.param_dec = chain(self.decoder.parameters())\n",
        "        self.param_vae = chain(self.encoder.parameters(), self.decoder.parameters())\n",
        "        self.param_disc = chain(self.discriminator.parameters())\n",
        "        \n",
        "    def forward(self, text, length=None, teacherForcing=False, use_discriminator=True, tfr=0.5, loss=False):\n",
        "        '''\n",
        "        text: tensor of input text\n",
        "        length: Maximum length of output\n",
        "        teacherForcing: force teacher forcing\n",
        "        use_discriminator: Run the discriminator model on the output\n",
        "        tfr: teacher forcing ratio\n",
        "        loss: also returns mu and sigma for the loss calculations\n",
        "        \n",
        "        returns: output tensor, context tensor, discriminators context tensor\n",
        "        '''\n",
        "        if length is None:\n",
        "            length = text.shape[0]\n",
        "        else:\n",
        "            length = min(text.shape[0], length)\n",
        "        batch_size = text.shape[1]\n",
        "        \n",
        "        # Encode the text\n",
        "        chat, latent, mu, sig = self.encoder(text)\n",
        "        \n",
        "        # Decode the text\n",
        "        if teacherForcing: # Chance for teacher forcing\n",
        "            outputs = torch.zeros(length, batch_size, self.vocab_size).to(self.device)\n",
        "            decText = text[0,:].unsqueeze(0)\n",
        "            hidden = None\n",
        "            for timestep in range(length):\n",
        "                output, hidden = self.decoder(decText, chat, latent, hidden=hidden)\n",
        "                outputs[timestep] = output\n",
        "                tf = random.random() < tfr\n",
        "                top1 = output.max(2)[1]\n",
        "                if tf:\n",
        "                    decText = text[timestep,:].unsqueeze(0)\n",
        "                else:\n",
        "                    decText = top1\n",
        "        else: # No teacher forcing\n",
        "            outputs, _ = self.decoder(text, chat, latent)\n",
        "            \n",
        "        # Discriminate the output\n",
        "        cstar = None\n",
        "        if use_discriminator:\n",
        "            discInputs = torch.zeros(length, batch_size).to(self.device)\n",
        "            for t in range(length):\n",
        "                discInputs[t] = outputs[t].max(1)[1]\n",
        "\n",
        "            cstar, _, _, _ = self.discriminator(discInputs.long(), usedAsDiscriminator=True)\n",
        "        \n",
        "        if loss:\n",
        "            return outputs, chat, cstar, mu, sig            \n",
        "        else:\n",
        "            return outputs, chat, cstar\n",
        "      \n",
        "    def generate_beam(self, text=None, context=None, latent=None, maxlen=50, beam=3, adaptive=False, unk=True, batch_size=1, min_len=None, continueStarter=False, rand=False):\n",
        "        '''\n",
        "        text: staring text\n",
        "        context: context vector\n",
        "        latent: latent vector\n",
        "        maxlen: maximum output size\n",
        "        beam: beam size\n",
        "        adaptive: derank undesirable output\n",
        "        unk: derank unk characters in output\n",
        "        batch_size: size of batch\n",
        "        min_length: suggested minimum length of output\n",
        "        continueStarter: Generate the context and latent based off of text\n",
        "        rand: Randomly generate the context\n",
        "        \n",
        "        returns: array of beams where each beam contains the following\n",
        "        (ongoing, probability, output text, input text, hidden, prev)\n",
        "        '''\n",
        "        # If not defined, starting text is just the start character\n",
        "        if text is None:\n",
        "            text = torch.Tensor([self.start_idx]).long().repeat(1, batch_size).to(self.device)\n",
        "            \n",
        "        # Generate the context vector\n",
        "        if context is None:\n",
        "            if continueStarter:\n",
        "                context, _, _, _ = self.encoder(text)\n",
        "            elif rand:\n",
        "                context = torch.randn((1,1)).to(self.device)\n",
        "            else:\n",
        "                context = torch.randint(1+1, (1,batch_size), dtype=torch.float).to(self.device)\n",
        "        \n",
        "        # Generate the latent vector\n",
        "        if latent is None:\n",
        "            if continueStarter:\n",
        "                latent = self.make_latent(batch_size)\n",
        "            else:\n",
        "                _, latent, _, _ = self.encoder(text)\n",
        "                \n",
        "        # Create the first beam, just containing the starting text\n",
        "        beams = []\n",
        "        ongoing = beam\n",
        "        for i in range(1):\n",
        "            outputs = []\n",
        "            for item in text.cpu().flatten():\n",
        "                outputs.append(item)\n",
        "            # (ongoing, probability, output text, input text, hidden, prev)\n",
        "            beams.append((True, 1, outputs, text, None, -1))\n",
        "            \n",
        "        first = False\n",
        "        sm = nn.Softmax(dim=1)\n",
        "        timestep = 0\n",
        "        # While there are at least 1 ongoing beam\n",
        "        while ongoing > 0 and timestep < maxlen:\n",
        "            timestep = timestep + 1\n",
        "            newBeams = []\n",
        "            \n",
        "            # For each beam in the beams list\n",
        "            for item in beams:\n",
        "                con, prob, outputs, text, hidden, prev = item\n",
        "                \n",
        "                # If continuing\n",
        "                if con:\n",
        "                    # Generate the probabilities for the next word\n",
        "                    output, newHidden = self.decoder(text, context, latent, hidden=hidden)\n",
        "                    output = sm(output[-1])\n",
        "                    tops = torch.topk(output, k=beam, dim=1)\n",
        "                    \n",
        "                    idxs = torch.topk(output, k=beam, dim=1)[1]\n",
        "                    vals = torch.topk(output, k=beam, dim=1)[0]\n",
        "                    \n",
        "                    # For all top probability words\n",
        "                    for k in range(len(idxs[0])):\n",
        "                        idx = idxs[0][k]\n",
        "                        \n",
        "                        newprob = prob * vals[0][k]\n",
        "                        \n",
        "                        # Derank output based on generated value\n",
        "                        if unk and (idx == self.unk_idx or idx == self.pad_idx):\n",
        "                            newprob = newprob * 0.50\n",
        "                        \n",
        "                        if first and adaptive and k > 0:\n",
        "                            newprob = newprob * 0.80\n",
        "                            \n",
        "                        if adaptive and prev == idx:\n",
        "                            newprob = newprob * 0.1\n",
        "                            \n",
        "                        if (not min_len is None) and text.shape[0] < min_len and (idx == self.eos_idx or idx == self.pad_idx):\n",
        "                            newprob = newprob * 0.01\n",
        "                        \n",
        "                        # Create the tensor for the next word\n",
        "                        nextWord = torch.Tensor([idx]).long().repeat(1, batch_size).to(self.device)\n",
        "                        \n",
        "                        newtext = torch.cat([text, nextWord], dim=0)\n",
        "                        \n",
        "                        newoutputs = copy.copy(outputs)\n",
        "                        newoutputs.append(nextWord)\n",
        "                        \n",
        "                        # If generated the end of sentence tag, then discontinue the beam\n",
        "                        newcon = con\n",
        "                        if idx == self.eos_idx or idx == self.pad_idx:\n",
        "                            newcon = False\n",
        "                            \n",
        "                        newBeams.append((newcon, newprob, newoutputs, newtext, newHidden, idx))\n",
        "                else:\n",
        "                    newBeams.append((con, prob, outputs, text, hidden, prev))\n",
        "            \n",
        "            # Sort the beams based on their probability \n",
        "            topBeams = [newBeams[0]]\n",
        "            for i in range(1, len(newBeams)):\n",
        "                _, prob, _, _, _, _ = newBeams[i]\n",
        "                \n",
        "                for j in range(len(topBeams)):\n",
        "                    if prob > topBeams[j][1]:\n",
        "                        temp = topBeams[j][1]\n",
        "                        topBeams.insert(j, newBeams[i])\n",
        "                        if len(topBeams) > beam:\n",
        "                            del topBeams[-1]\n",
        "                        break\n",
        "                if len(topBeams) < beam:\n",
        "                    topBeams.append(newBeams[i])\n",
        "                  \n",
        "            # Count the ongoing beams\n",
        "            beams = topBeams\n",
        "            ongoing = 0\n",
        "            for item in beams:\n",
        "                con, _, _, _, _, _ = item\n",
        "                if con:\n",
        "                    ongoing = ongoing + 1\n",
        "                    \n",
        "        return beams\n",
        "        \n",
        "    def generate(self, text=None, context=None, latent=None, max_len=50, batch_size=1):\n",
        "        '''\n",
        "        Generate random sentences without beamsearch.\n",
        "        Text: Starting text\n",
        "        context: desired context vector\n",
        "        latent: Desired latent vector\n",
        "        max_len: maximum length of generated sentences\n",
        "        batch_size: size of batch\n",
        "        \n",
        "        returns: tensor of generated words contexted on text, context, and latent\n",
        "        '''\n",
        "        # If not provided, setup starting word\n",
        "        if text is None:\n",
        "            text = torch.Tensor([self.start_idx]).long().repeat(1, batch_size).to(self.device)\n",
        "\n",
        "        # If not provided, setup random context\n",
        "        if context is None:\n",
        "            context = torch.randint(1+1, (batch_size, self.context_dim), dtype=torch.float).to(device)\n",
        "        \n",
        "        # If not provided, setup random latent\n",
        "        if latent is None:\n",
        "            latent = self.make_latent(batch_size)\n",
        "        \n",
        "        outputs = torch.zeros(max_len, batch_size, 1).to(self.device)\n",
        "        decText = text[0,:].unsqueeze(0)\n",
        "        hidden = None\n",
        "        \n",
        "        # Generate max_len words and add them to the list of words\n",
        "        for timestep in range(1, max_len):\n",
        "            output, hidden = self.decoder(decText, context, latent, hidden=hidden)\n",
        "            top1 = output.max(2)[1]\n",
        "            decText = top1\n",
        "            outputs[timestep] = top1.unsqueeze(2).squeeze(0)\n",
        "            \n",
        "        return outputs.squeeze(2).long(), context, latent    \n",
        "    \n",
        "    def make_latent(self, batch=1):\n",
        "        '''\n",
        "        Sample a random latent vector from the normal distribution\n",
        "        '''\n",
        "        latent = torch.abs(torch.randn(batch, self.latent_dim).to(device))\n",
        "        return latent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sO2_hchwmIt",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train the System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URIPx5CtKJF2",
        "colab": {}
      },
      "source": [
        "class TrainVAE():\n",
        "    '''\n",
        "    Training module for the entire system.\n",
        "    '''\n",
        "    def __init__(self, device, model, lmbda=1, blmbda=0.1, clmbda=0.4, clip=5, pad_idx=1):\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # Optimizers for different parts of the model\n",
        "        self.optim_enc  = optim.Adam(self.model.param_enc)\n",
        "        self.optim_dec  = optim.Adam(self.model.param_dec)\n",
        "        self.optim_vae  = optim.Adam(self.model.param_vae)\n",
        "        self.optim_disc = optim.Adam(self.model.param_disc)\n",
        "        \n",
        "        self.clip = clip\n",
        "        self.lmbda = lmbda\n",
        "        self.blmbda = blmbda\n",
        "        self.clmbda = clmbda\n",
        "        self.pad_idx = pad_idx\n",
        "        \n",
        "    def train(self, train_iterator, valid_iterator, epocs=1, trainDisc=True, trainVAE=True, trainGen=True, max_len=100, valid=True):\n",
        "        '''\n",
        "        trainDisc: boolean, train the discriminator\n",
        "        trainVAE: boolean, train the VAE\n",
        "        trainGen: boolean, train the Generator\n",
        "        max_len: integer, maximum length of a sentence\n",
        "        valid: boolean, evaluate on the valid set\n",
        "        '''\n",
        "        best_valid_loss = float('inf')\n",
        "        for epoch in range(epocs):\n",
        "\n",
        "            train_loss = self.train_epoc(train_iterator, epoch, trainDisc, trainVAE, trainGen, max_len)\n",
        "            if valid:\n",
        "                valid_loss = self.evaluate(valid_iterator)\n",
        "\n",
        "                if valid_loss < best_valid_loss:\n",
        "                    best_valid_loss = valid_loss\n",
        "\n",
        "            print(f'Epoch: {epoch+1:02}')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "            if valid:\n",
        "                print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        \n",
        "    def train_disc(self, data, label):\n",
        "        '''\n",
        "        train_disc trains the discriminator module\n",
        "        '''\n",
        "        self.optim_disc.zero_grad()\n",
        "        \n",
        "        # Compute the Discriminator Loss\n",
        "        context, _, _, _ = self.model.discriminator(data)\n",
        "        context = context.squeeze(1)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = self.loss_fn(context, label)\n",
        "\n",
        "        # Compute the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.param_disc, self.clip)\n",
        "\n",
        "        self.optim_disc.step() # update the parameters\n",
        "        self.optim_disc.zero_grad()\n",
        "        \n",
        "        return loss.item()\n",
        "        \n",
        "    def train_vae(self, data, max_len=100, target=None):\n",
        "        '''\n",
        "        train_vae trains the VAE module\n",
        "        '''\n",
        "        batch_size = data.shape[1]\n",
        "        loss = 0\n",
        "        \n",
        "        # if not defined, generate the target sentence\n",
        "        if target is None:\n",
        "            target = makeTarget(data, device=self.device, vocab_len=VOCAB_SIZE, pad_idx=self.pad_idx, max_len=max_len)\n",
        "            \n",
        "        # Compute VAE Loss\n",
        "        # Split sentences up into max_len sizes\n",
        "        self.optim_vae.zero_grad()\n",
        "        mini_batch_size = max_len\n",
        "        for i in range(0, data.shape[0], mini_batch_size):\n",
        "            en = i + mini_batch_size\n",
        "            if en > data.shape[0]:\n",
        "                en = data.shape[0]\n",
        "            if i != en:\n",
        "                loss += self.train_vae_iter(data[i:en], target[i:en], max_len)\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "        return loss\n",
        "        \n",
        "    def train_vae_iter(self, data, target, max_len):\n",
        "        '''\n",
        "        train_vae_iter computes the loss for each sentence\n",
        "        '''\n",
        "        output, chat, context, mu, sig = self.model(data, length=max_len, teacherForcing=True, loss=True)\n",
        "        lo = self.clmbda * torch.mean(0.5 * torch.sum(torch.exp(sig) + mu**2 - 1 - sig, dim=1))\n",
        "        trgt = target\n",
        "        # Compute the loss\n",
        "        lossDec = self.criterion(output.view(-1, VOCAB_SIZE), trgt.view(-1))  \n",
        "        lossEnc = self.loss_fn(chat, context) * self.lmbda\n",
        "        lossGen = lossDec + lo + lossEnc\n",
        "        # Compute the gradients\n",
        "        lossGen.backward()\n",
        "\n",
        "        # Clip the gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.param_vae, self.clip)\n",
        "\n",
        "        self.optim_vae.step() # update the parameters\n",
        "        self.optim_vae.zero_grad()\n",
        "        \n",
        "        return lossGen.item()\n",
        "        \n",
        "    def train_gen(self, data, label, max_len=100, target=None):\n",
        "        '''\n",
        "        train_gen trains the generator module\n",
        "        '''\n",
        "        if target is None:\n",
        "            target = makeTarget(data, device=self.device, vocab_len=VOCAB_SIZE, pad_idx=self.pad_idx, max_len=max_len)\n",
        "            \n",
        "        batch_size = data.shape[1]\n",
        "        if max_len > data.shape[0]:\n",
        "            max_len = data.shape[0]\n",
        "            \n",
        "        # Calculate the generator loss on the generator\n",
        "        loss = self.gen_1(data,label, max_len)\n",
        "        \n",
        "        # Calculate the generator loss on the encoder\n",
        "        loss += self.gen_3(data, label, max_len)\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        # Calculate the generator loss on the decoder\n",
        "        mini_batch_size = max_len\n",
        "        for i in range(0, data.shape[0], mini_batch_size):\n",
        "            en = i + mini_batch_size\n",
        "            if en > data.shape[0]:\n",
        "                en = data.shape[0]\n",
        "            if i != en:\n",
        "                loss += self.gen_2(data[i:en], target[i:en], max_len)\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def gen_1(self, data, label, max_len):\n",
        "        '''\n",
        "        Calculate the generator loss on the discriminator\n",
        "        '''\n",
        "        batch_size = data.shape[1]\n",
        "        # Compute Loss for generator-discriminator\n",
        "        self.optim_disc.zero_grad()\n",
        "\n",
        "        x, chat, _ = self.model.generate(batch_size=batch_size, max_len=max_len)\n",
        "        cstar_gen, _, _, _ = self.model.discriminator(x)\n",
        "        cstar_real, _, _, _ = self.model.discriminator(data)\n",
        "        \n",
        "        # compute losses\n",
        "        lo = F.log_softmax(cstar_gen, dim=1)\n",
        "        ent = -lo.mean()\n",
        "        lossDisc = self.loss_fn(cstar_real.squeeze(1), label) + (self.loss_fn(cstar_gen, chat) + self.blmbda*ent) * self.blmbda\n",
        "\n",
        "        # Compute the gradients\n",
        "        lossDisc.backward()\n",
        "\n",
        "        # Clip the gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.param_disc, self.clip)\n",
        "\n",
        "        # update disc\n",
        "        self.optim_disc.step() # update the parameters\n",
        "        self.optim_disc.zero_grad()\n",
        "\n",
        "        return lossDisc.item()\n",
        "    \n",
        "    def gen_2(self, data, target, max_len):\n",
        "        '''\n",
        "        Calculate the generator loss on the Decoder\n",
        "        '''\n",
        "        batch_size = data.shape[1]\n",
        "        \n",
        "        output, chat, context, mu, sig = self.model(data, length=max_len, teacherForcing=False, loss=True)\n",
        "        lo = torch.abs(self.clmbda * torch.mean(0.5 * torch.sum(torch.exp(sig) + mu**2 - 1 - sig, 1)))\n",
        "        \n",
        "        x, cgen, lat = self.model.generate(batch_size=batch_size, max_len=max_len)\n",
        "        \n",
        "        \n",
        "        _, zenc, _, _ = self.model.encoder(x)\n",
        "        cdisc, _, _, _ = self.model.discriminator(x)\n",
        "        \n",
        "        # compute losses\n",
        "        lossA = self.criterion(output.view(-1, VOCAB_SIZE), target.view(-1))  \n",
        "        lossB = self.loss_fn(chat, context) * self.lmbda\n",
        "        lossVAE = lossA + lo + lossB\n",
        "        \n",
        "        lossC = self.loss_fn(cdisc, cgen)\n",
        "        lossL = F.mse_loss(zenc, lat)\n",
        "        \n",
        "        lossGen = lossVAE + lossC*self.blmbda + lossL*self.blmbda\n",
        "        \n",
        "        # back Prop loss\n",
        "        lossGen.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.param_vae, self.clip)\n",
        "        self.optim_dec.step()\n",
        "        self.optim_dec.zero_grad()\n",
        "        \n",
        "        return lossGen.item()\n",
        "        \n",
        "    def gen_3(self, data, label, max_len):\n",
        "        '''\n",
        "        Calculate the generator loss on the Encoder\n",
        "        '''\n",
        "        batch_size = data.shape[1]\n",
        "        \n",
        "        # Compute losses\n",
        "        _, chat, cstar, mu, sig = self.model(data, length=(max_len), teacherForcing=False, loss=True)\n",
        "        lo = torch.abs(self.clmbda * torch.mean(0.5 * torch.sum(torch.exp(sig) + mu**2 - 1 - sig, 1)))\n",
        "        \n",
        "        context, _, _, _ = self.model.encoder(data)\n",
        "        context = context.squeeze(1)\n",
        "        \n",
        "        lossEnc = lo + self.loss_fn(chat, cstar) + self.loss_fn(context,label)\n",
        "        \n",
        "        # backprop the loss\n",
        "        lossEnc.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.param_enc, self.clip)\n",
        "        self.optim_enc.step()\n",
        "        self.optim_enc.zero_grad()      \n",
        "        return lossEnc.item()\n",
        "        \n",
        "    def train_epoc(self, iterator, epoch, trainDisc=True, trainVAE=True, trainGen=True, max_len=100):\n",
        "        '''\n",
        "        train_epoc trains one epoch of the model\n",
        "        '''\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        for i, batch in enumerate(iterator):\n",
        "            torch.cuda.empty_cache()\n",
        "            data, label = batch.text.to(self.device), batch.label.to(self.device)\n",
        "            \n",
        "            target = makeTarget(data, device=self.device, vocab_len=VOCAB_SIZE, pad_idx=self.pad_idx, max_len=max_len)\n",
        "            lossD = 0\n",
        "            lossV = 0\n",
        "            lossG = 0\n",
        "            \n",
        "            # Train the discriminator\n",
        "            if trainDisc:\n",
        "                lossD = self.train_disc(data, label)\n",
        "            # Train the VAE using teacher forcing\n",
        "            if trainVAE:\n",
        "                lossV += self.train_vae(data, max_len=max_len, target=target)\n",
        "            # Train the Generator\n",
        "            if trainGen:\n",
        "                lossG += self.train_gen(data, label, max_len=max_len, target=target)\n",
        "                    \n",
        "            loss = lossD + lossV + lossG\n",
        "            epoch_loss += loss\n",
        "                \n",
        "            if i % 100 == 0:\n",
        "                print(\"epoch\", epoch, \"iter\", i, \"loss\", loss)\n",
        "                if trainDisc:\n",
        "                    print(\"DISC loss\", lossD)\n",
        "                if trainVAE:\n",
        "                    print(\"VAE loss\", lossV)\n",
        "                if trainGen:\n",
        "                    print(\"Gen loss\", lossG)\n",
        "                torch.save(self.model.state_dict(), VAE_PATH)\n",
        "                \n",
        "        return epoch_loss / len(iterator)\n",
        "    \n",
        "        \n",
        "    def evaluate(self, iterator):\n",
        "        '''\n",
        "        Evaluate the losses on the components\n",
        "        '''\n",
        "        self.model.eval()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(iterator):\n",
        "                data, label = batch.text.to(self.device), batch.label.to(self.device)\n",
        "                max_len = data.shape[0]\n",
        "                batch_size = data.shape[1]\n",
        "                target = makeTarget(data, device=self.device, vocab_len=VOCAB_SIZE, pad_idx=self.pad_idx, max_len=max_len)\n",
        "\n",
        "                # Compute loss for Discriminator\n",
        "                context, _, _, _ = self.model.discriminator(data)\n",
        "                context = context.squeeze(1)\n",
        "                lossD = self.loss_fn(context, label)\n",
        "                lossD = lossD.item()\n",
        "                \n",
        "                # Compute VAE Loss\n",
        "                output, chat, context, mu, sig = self.model(data, teacherForcing=False, loss=True)\n",
        "                lo = torch.abs(self.clmbda * torch.mean(0.5 * torch.sum(torch.exp(sig) + mu**2 - 1 - sig, 1)))\n",
        "        \n",
        "                lossDec = self.criterion(output.view(-1, VOCAB_SIZE), target.view(-1))  \n",
        "                lossEnc = self.loss_fn(chat, context) * self.lmbda\n",
        "                lossG = lossDec + lo + lossEnc\n",
        "                lossG = lossG.item()\n",
        "\n",
        "                # Compute Generator Loss\n",
        "                output, chat, context, mu, sig = self.model(data, teacherForcing=False, loss=True)\n",
        "                lo = torch.abs(self.clmbda * torch.mean(0.5 * torch.sum(torch.exp(sig) + mu**2 - 1 - sig, 1)))\n",
        "        \n",
        "                x, cgen, lat = self.model.generate(batch_size=batch_size, max_len=max_len)\n",
        "\n",
        "                cenc, zenc, _, _ = self.model.encoder(x)\n",
        "                cdisc, _, _, _ = self.model.discriminator(x)\n",
        "\n",
        "                lossA = self.criterion(output.view(-1, VOCAB_SIZE), target.view(-1))  \n",
        "                lossB = self.loss_fn(chat, context) * self.lmbda\n",
        "                lossVAE = lossA + lo + lossB\n",
        "\n",
        "                lossC = self.loss_fn(cdisc, cgen)\n",
        "                lossL = F.mse_loss(zenc, lat)\n",
        "\n",
        "                lossGen = lossVAE + lossC*self.blmbda + lossL*self.blmbda\n",
        "                lossGen = lossGen.item()\n",
        "\n",
        "                # Count and display the loss\n",
        "                loss = lossD + lossG + lossGen\n",
        "                epoch_loss += lossD + lossG + lossGen\n",
        "                \n",
        "                if i % 100 == 0:\n",
        "                    print(\"iter\", i, \"Total loss\", loss, \"Discriminator loss\", lossD, \"VAE loss\", lossG, \"Generator loss\", lossGen)\n",
        "              \n",
        "        return epoch_loss / len(iterator)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEq7VopAiP6",
        "colab_type": "text"
      },
      "source": [
        "### Create and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qdsrPqTMQAcH",
        "colab": {}
      },
      "source": [
        "discriminator = RNNBinaryEncoder(VOCAB_SIZE_DISC, EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_DIM, 0, device).to(device)\n",
        "model = EncDec(device, discriminator, INPUT_DIM,  EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_DIM, LATENT_DIM, pad_idx=PAD_IDX).to(device)\n",
        "model.load_state_dict(torch.load(VAE_PATH))\n",
        "\n",
        "trainer = TrainVAE(device, model, lmbda=0.1, blmbda=0.1)\n",
        "\n",
        "print(\"Starting to train the Discriminator\")\n",
        "trainer.train(disc_train_iterator, disc_valid_iterator, trainDisc=True, trainVAE=False, trainGen=False, max_len=30, epocs=3, valid=False)\n",
        "\n",
        "print(\"Starting to train the VAE\")\n",
        "trainer.train(disc_train_iterator, disc_valid_iterator, trainDisc=False, trainVAE=True, trainGen=False, max_len=30, epocs=3, valid=False)\n",
        "\n",
        "print(\"Starting to train the Generator/Encoder and the Discriminator\")\n",
        "trainer.train(disc_train_iterator, disc_valid_iterator, trainDisc=False, trainVAE=False, trainGen=True, max_len=30, epocs=3, valid=False)\n",
        "\n",
        "print(\"Finding loss of final model\")\n",
        "test_loss = trainer.evaluate(disc_test_iterator)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "print(\"Done!\")\n",
        "\n",
        "# Save the model's parameters to file\n",
        "torch.save(model.state_dict(), VAE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bkKHIIrwYp0",
        "colab_type": "text"
      },
      "source": [
        "## 4. Test the system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Xr1yJ0gZK-C",
        "colab": {}
      },
      "source": [
        "# Load the model from file\n",
        "discriminator = RNNBinaryEncoder(VOCAB_SIZE_DISC, EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_DIM, 0, device).to(device)\n",
        "model = EncDec(device, discriminator, INPUT_DIM,  EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_DIM, LATENT_DIM).to(device)\n",
        "model.load_state_dict(torch.load(VAE_PATH))\n",
        "\n",
        "discriminator.eval()\n",
        "model.eval()\n",
        "\n",
        "# Define several variables\n",
        "pad = torch.Tensor(lookup_indexes([\"<pad>\"])).long().reshape((1,1)).to(device)\n",
        "unk = torch.Tensor(lookup_indexes([\"<unk>\"])).long().reshape((1,1)).to(device)\n",
        "\n",
        "# Create some context vectors\n",
        "pos = 1\n",
        "neg = -1\n",
        "neu = 0\n",
        "ptep = 0.5\n",
        "ntep = -0.5\n",
        "\n",
        "contextP = torch.Tensor([pos]).reshape((1,1)).to(device)\n",
        "contextN = torch.Tensor([neg]).reshape((1,1)).to(device)\n",
        "contextNEU = torch.Tensor([neu]).reshape((1,1)).to(device)\n",
        "contextPT = torch.Tensor([ptep]).reshape((1,1)).to(device)\n",
        "contextNT = torch.Tensor([ntep]).reshape((1,1)).to(device)\n",
        "\n",
        "contextRand = torch.randn((1,1)).to(device)\n",
        "\n",
        "# Create some starter tensors\n",
        "starter = torch.randint(INPUT_DIM, (1,1), dtype=torch.long).to(device)\n",
        "starterInit = torch.Tensor(lookup_indexes([\"<init>\"])).long().reshape((1,1)).to(device)\n",
        " \n",
        "# Generate context and latent vectors from example sentences\n",
        "contextR1, latentR1, _, _ = model.encoder(makeStarter(\"i went to see this movie last week .\", device))\n",
        "contextR2, latentR2, _, _ = model.encoder(makeStarter(\"i loved this movie !\", device))\n",
        "contextNE, latentNE, _, _ = model.encoder(makeStarter(\"this was the abolute worst movie ever do not see it i hate this movie it was terrible\", device))\n",
        "contextPE, latentPE, _, _ = model.encoder(makeStarter(\"good i love this movie . it was fantastic . the acting was excellent , the score was great\", device))\n",
        "  \n",
        "latentZ = torch.zeros(1, LATENT_DIM).to(device)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krn8_H-WCyoe",
        "colab_type": "text"
      },
      "source": [
        "# For a demonstration use this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fv3YxTLHZcdx",
        "colab": {}
      },
      "source": [
        "# NOTE: punctuation needs to be sepearte from words by a space.\n",
        "# EXAMPLE: \"I loved it!\" should be \"I loved it !\"\n",
        "\n",
        "# For Demo, use any of the following contexts, latents, and starters\n",
        "# Positive contexts\n",
        "# contextP, contextPT, contextPE, contextR2\n",
        "\n",
        "# Negative contexts\n",
        "# contextN, contextNT, contextNE\n",
        "\n",
        "# Positive Latent Vectors\n",
        "# latentPE, latentR2\n",
        "\n",
        "# Negative Latent Vectors\n",
        "# LatentNE\n",
        "\n",
        "# Emtpy starter\n",
        "# starterInit\n",
        "\n",
        "# Custom/Random starters\n",
        "# starterCustum, starterRand\n",
        "starterCustom = makeStarter(\"i went to see\", device)\n",
        "starterRand = makeStarter(1, device, random=True, starter=True)\n",
        "\n",
        "# generate_beam inputs:\n",
        "# generate_beam(starting text, context vector, latent vector, max length, beam size, use adaptive serch, derank \"unk\", suggested min length, random context)\n",
        "# Note: All values are optional\n",
        "\n",
        "# Generate a beam of results\n",
        "output = model.generate_beam(text=starterInit, context=contextP, latent=latentP, maxlen=20, beam=15, adaptive=True, unk=True, min_len=5, rand=False)\n",
        "#output = model.generate_beam(text=starterInit, context=None, latent=None, maxlen=20, beam=15, adaptive=True, unk=True, min_len=5)\n",
        "\n",
        "# Display the beam of results\n",
        "for beam in output:\n",
        "    print(\" \".join(lookup_words(beam[2])))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}